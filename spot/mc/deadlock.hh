// -*- coding: utf-8 -*-
// Copyright (C) 2015, 2016, 2017 Laboratoire de Recherche et
// Developpement de l'Epita
//
// This file is part of Spot, a model checking library.
//
// Spot is free software; you can redistribute it and/or modify it
// under the terms of the GNU General Public License as published by
// the Free Software Foundation; either version 3 of the License, or
// (at your option) any later version.
//
// Spot is distributed in the hope that it will be useful, but WITHOUT
// ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
// or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public
// License for more details.
//
// You should have received a copy of the GNU General Public License
// along with this program.  If not, see <http://www.gnu.org/licenses/>.

#pragma once

#include <stdlib.h>
#include <atomic>
#include <chrono>
#include <spot/bricks/brick-hashset>
#include <spot/kripke/kripke.hh>
#include <spot/misc/common.hh>
#include <spot/misc/fixpool.hh>
#include <spot/misc/timer.hh>
#include <thread>
#include <vector>

#include <algorithm>
#include <functional>
#include <string>

#include <spot/dmc/message.hh>
#include <spot/dmc/process.hh>

// Ignore all errors generated by the compiler when including the mpi library.
#pragma GCC diagnostic push
#pragma GCC diagnostic ignored "-Wsuggest-override"
#pragma GCC diagnostic ignored "-Wzero-as-null-pointer-constant"
#include <mpi.h>
#pragma GCC diagnostic pop

#define TRESHOLD 10

#define SEND_ALL_STATES 0

namespace spot
{
/// \brief This object is returned by the algorithm below
struct SPOT_API deadlock_stats
{
  unsigned states;       ///< \brief Number of states visited
  unsigned transitions;  ///< \brief Number of transitions visited
  unsigned instack_dfs;  ///< \brief Maximum DFS stack
  bool has_deadlock;     ///< \brief Does the model contains a deadlock
  unsigned walltime;     ///< \brief Walltime for this thread in ms
};

/// \brief This class aims to explore a model to detect wether it
/// contains a deadlock. This deadlock detection performs a DFS traversal
/// sharing information shared among multiple threads.
template <typename State, typename SuccIterator, typename StateHash,
          typename StateEqual>
class swarmed_deadlock
{
  /// \brief Describes the status of a state
  enum st_status
  {
    UNKNOWN = 1,  // First time this state is discoverd by this thread
    OPEN = 2,     // The state is currently processed by this thread
    CLOSED = 4,   // All the successors of this state have been visited
  };

  /// \brief Describes the structure of a shared state
  struct deadlock_pair
  {
    State st;     ///< \brief the effective state
    int* colors;  ///< \brief the colors (one per thread)
  };

  /// \brief The haser for the previous state.
  struct pair_hasher
  {
    pair_hasher(const deadlock_pair&)
    {}

    pair_hasher() = default;

    brick::hash::hash128_t hash(const deadlock_pair& lhs) const
    {
      StateHash hash;
      // Not modulo 31 according to brick::hashset specifications.
      unsigned u = hash(lhs.st) % (1 << 30);
      return {u, u};
    }

    bool equal(const deadlock_pair& lhs, const deadlock_pair& rhs) const
    {
      StateEqual equal;
      return equal(lhs.st, rhs.st);
    }
  };

 public:
  ///< \brief Shortcut to ease shared map manipulation
  using shared_map = brick::hashset::FastConcurrent<deadlock_pair, pair_hasher>;

  swarmed_deadlock(kripkecube<State, SuccIterator>& sys, shared_map& map,
                   unsigned tid, std::atomic<bool>& stop,
                   std::atomic<bool>& finale)
      : sys_(sys),
        tid_(tid),
        map_(map),
        nb_th_(std::thread::hardware_concurrency()),
        p_(sizeof(int) * std::thread::hardware_concurrency()),
        stop_(stop),
        finale_(finale)
  {
    SPOT_ASSERT(is_a_kripkecube(sys));
  }

  virtual ~swarmed_deadlock()
  {
    while (!todo_.empty())
      {
        sys_.recycle(todo_.back().it, tid_);
        todo_.pop_back();
      }
  }

  void setup()
  {
    tm_.start("DFS thread " + std::to_string(tid_));
  }

  bool push(State s)
  {
    // Prepare data for a newer allocation
    int* ref = (int*)p_.allocate();
    for (unsigned i = 0; i < nb_th_; ++i)
      ref[i] = UNKNOWN;

    // Try to insert the new state in the shared map.
    auto it = map_.insert({s, ref});
    bool b = it.isnew();

    // Insertion failed, delete element
    // FIXME Should we add a local cache to avoid useless allocations?
    if (!b)
      p_.deallocate(ref);

    // The state has been mark dead by another thread
    for (unsigned i = 0; !b && i < nb_th_; ++i)
      if (it->colors[i] == static_cast<int>(CLOSED))
        return false;

    // The state has already been visited by the current thread
    if (it->colors[tid_] == static_cast<int>(OPEN))
      return false;

    // Keep a ptr over the array of colors
    refs_.push_back(it->colors);

    // Mark state as visited.
    it->colors[tid_] = OPEN;
    ++states_;
    return true;
  }

  bool pop()
  {
    // Track maximum dfs size
    dfs_ = todo_.size() > dfs_ ? todo_.size() : dfs_;

    // Don't avoid pop but modify the status of the state
    // during backtrack
    refs_.back()[tid_] = CLOSED;
    refs_.pop_back();
    return true;
  }

  void finalize()
  {
    stop_ = true;
    tm_.stop("DFS thread " + std::to_string(tid_));
  }

  unsigned states()
  {
    return states_;
  }

  unsigned transitions()
  {
    return transitions_;
  }

  void run()
  {
    setup();
    State initial = sys_.initial(tid_);
    if (SPOT_LIKELY(push(initial)))
      {
        todo_.push_back({initial, sys_.succ(initial, tid_), transitions_});
      }
    while (!todo_.empty() && !stop_.load(std::memory_order_relaxed))
      {
        if (todo_.back().it->done())
          {
            if (SPOT_LIKELY(pop()))
              {
                deadlock_ = todo_.back().current_tr == transitions_;
                if (deadlock_)
                  break;
                sys_.recycle(todo_.back().it, tid_);
                todo_.pop_back();
              }
          }
        else
          {
            ++transitions_;
            State dst = todo_.back().it->state();

            if (SPOT_LIKELY(push(dst)))
              {
                todo_.back().it->next();
                todo_.push_back({dst, sys_.succ(dst, tid_), transitions_});
              }
            else
              {
                todo_.back().it->next();
              }
          }
      }
    finalize();
  }

  bool has_deadlock()
  {
    return deadlock_;
  }

  unsigned walltime()
  {
    return tm_.timer("DFS thread " + std::to_string(tid_)).walltime();
  }

  deadlock_stats stats()
  {
    return {states(), transitions(), dfs_, has_deadlock(), walltime()};
  }

  /**************************************************************************
   *                                MPI                                     *
   **************************************************************************/

  /* This function directly marks the states as closed
     and does not increase the number of states visited.
     Moreover it does not update the refs stack */
  bool push_and_close(State s)
  {
    // Prepare data for a newer allocation
    int* ref = (int*)p_.allocate();
    for (unsigned i = 0; i < nb_th_; ++i)
      ref[i] = UNKNOWN;

    // Try to insert the new state in the shared map.
    auto it = map_.insert({s, ref});
    bool b = it.isnew();

    // Insertion failed, delete element
    // FIXME Should we add a local cache to avoid useless allocations?
    if (!b)
      p_.deallocate(ref);

    // The state has been mark dead by another thread
    for (unsigned i = 0; !b && i < nb_th_; ++i)
      if (it->colors[i] == static_cast<int>(CLOSED))
        return false;

    // Mark state as visited.
    it->colors[tid_] = CLOSED;
    return true;
  }

  /* Given that we only receive the last state we have pop,
     this function marks all successor states as dead. */
  void recurssively_close(State s)
  {
    if (!SEND_ALL_STATES)
      {
        // The new DFS stack
        std::vector<todo__element> todo;

        if (SPOT_LIKELY(push_and_close(s)))
          {
            /* Do not use to make benchmarks.
               Commenting this line allows to decrease the treshold. */
            // shared_states_.push_back(s);
            todo.push_back({s, sys_.succ(s, tid_), 0});

            /* Launches a second DFS course
               to close all successors from the state received. */
            while (!todo.empty())
              {
                // No need for pop since we don't keep a ptr over the array of
                // colors.
                if (todo.back().it->done())
                  {
                    sys_.recycle(todo.back().it, tid_);
                    todo.pop_back();
                  }

                else
                  {
                    State dst = todo.back().it->state();

                    if (SPOT_LIKELY(push_and_close(dst)))
                      {
                        todo.back().it->next();
                        todo.push_back({dst, sys_.succ(dst, tid_), 0});
                      }

                    else
                      {
                        todo.back().it->next();
                      }
                  }
              }
          }
      }

    else
      push_and_close(s);
  }

  void distributed_run(spot::process* proc)
  {
    int size = proc->get_size();
    int rank = proc->get_rank();
    unsigned deadlock = 0;
    /*unsigned end = 1;*/
    int deadlock_tag = 1;
    /*int end_tag = 3;*/
    int* persistent_send_buffer = nullptr;
    int* persistent_recv_buffer = nullptr;
    int lenght = -1;
    int* state_recv = nullptr;
    /*int* end_buf = new int[100]();*/
    int state_tag = 2;
    /*int cpt = 0;*/
    // int cpt_pop = 0;

    spot::message<unsigned>* deadlock_msg = new spot::message<unsigned>(
        &deadlock, nullptr, 1, deadlock_tag, MPI_UNSIGNED, size);
    spot::message<int>* state_msg = nullptr;
    /*spot::message<unsigned>* end_msg =
        new spot::message<unsigned>(&end, nullptr, 1, end_tag, MPI_UNSIGNED, size);*/

    setup();
    State initial = sys_.initial(tid_);

    if (SPOT_LIKELY(push(initial)))
      {
        todo_.push_back({initial, sys_.succ(initial, tid_), transitions_});
        lenght = todo_.back().s[1] + 2;
        persistent_send_buffer = new int[lenght];
        persistent_recv_buffer = new int[lenght];

        /* We have a unique message structure. each send/receive is
           done in a persistent buffer which allows to have
           a single object and to use persistent communication.
           The result is then copied into a new buffer and
           stored in the hash table. */
        state_msg = new spot::message<int>(persistent_send_buffer,
                                           persistent_recv_buffer, lenght,
                                           state_tag, MPI_INT, size);

        for (int id = rank + 1; (id % size) != rank; id = id + 1)
          state_msg->init_persistent_send(id % size);
      }

    while (!todo_.empty() && !stop_.load(std::memory_order_relaxed))
      {
        /*         end_msg->async_probe(MPI_ANY_SOURCE);

                if (end_msg->get_flag())
                  {
                    goto safe_exit;
                  }
         */
        /* This function acts like a mailbox.
           It notifies the arrival of a message by putting the flag at 1. */

        if (tid_ == 0)
          {
            deadlock_msg->async_probe(MPI_ANY_SOURCE);

            if (deadlock_msg->get_flag())
              {
                deadlock_ = true;
                goto safe_exit;
              }

            /*             end_msg->async_probe(MPI_ANY_SOURCE);

                        if (end_msg->get_flag())
                          {
                            cpt++;
                            goto safe_exit;
                          } */

            state_msg->async_probe(MPI_ANY_SOURCE);

            if (state_msg->get_flag())
              {
                /* This function retrieves the message
                  associated with the notification. */
                state_msg->match_async_recv();

                /* do not release because the pointer is stored directly
                   in the shared table. Instead use the shared_states_ vector to
                   store a copy of the pointer and release the memory later */
                state_recv = new int[lenght];
                std::copy_n(persistent_recv_buffer, lenght, state_recv);
                push_and_close((State)state_recv);
              }

            continue;
          }

        if (todo_.back().it->done())
          {
            if (SPOT_LIKELY(pop()))
              {
                deadlock_ = todo_.back().current_tr == transitions_;

                if (deadlock_)
                  {
                    /* Asynchronously notify
                       the other processes of the deadlock detection. */
                    for (int id = rank + 1; (id % size) != rank; id = id + 1)
                      deadlock_msg->async_send(id % size);
                    break;
                  }

                // cpt_pop++;

                /* Sends the last state up according to the threshold
                   in order to limit the number of messages sent. */
                if ((todo_.size() % TRESHOLD == 0) || SEND_ALL_STATES)
                  {
                    std::copy_n(todo_.back().s, lenght, persistent_send_buffer);

                    /* As we send a lot of data I use a communication request by
                       neighbors which allows not to congest the request. */
                    for (int id = rank + 1; (id % size) != rank; id = id + 1)
                      state_msg->start_persistent_send(id % size);

                    // cpt_pop = 0;
                  }

                sys_.recycle(todo_.back().it, tid_);
                todo_.pop_back();
              }
          }

        else
          {
            ++transitions_;
            State dst = todo_.back().it->state();

            if (SPOT_LIKELY(push(dst)))
              {
                todo_.back().it->next();
                todo_.push_back({dst, sys_.succ(dst, tid_), transitions_});
              }

            else
              {
                todo_.back().it->next();
              }
          }
      }

    for (int id = rank + 1; (id % size) != rank; id = id + 1)
      deadlock_msg->async_send(id % size);

  safe_exit:
    finalize();

    while (finale_.load() == false)
      {
        if (tid_ == 0)
          {
            proc->barrier();
            finale_= true;
          }
      }
  }

 private:
  struct todo__element
  {
    State s;
    SuccIterator* it;
    unsigned current_tr;
  };
  kripkecube<State, SuccIterator>& sys_;  ///< \brief The system to check
  std::vector<todo__element> todo_;       ///< \brief The DFS stack
  unsigned transitions_ = 0;              ///< \brief Number of transitions
  unsigned tid_;                          ///< \brief Thread's current ID
  shared_map map_;                        ///< \brief Map shared by threads
  spot::timer_map tm_;                    ///< \brief Time execution
  unsigned states_ = 0;                   ///< \brief Number of states
  unsigned dfs_ = 0;                      ///< \brief Maximum DFS stack size
  /// \brief Maximum number of threads that can be handled by this algorithm
  unsigned nb_th_ = 0;
  fixed_size_pool<pool_type::Unsafe> p_;  ///< \brief State Allocator
  bool deadlock_ = false;                 ///< \brief Deadlock detected?
  std::atomic<bool>& stop_;               ///< \brief Stop-the-world boolean
  std::atomic<bool>& finale_;
  /// \brief Stack that grows according to the todo stack. It avoid multiple
  /// concurent access to the shared map.
  std::vector<int*> refs_;

  ///< \brief Keep a copy of the storage addresses of the received states
  /// to be able to free the memory.
  /// Please comment if you plan to do benchmarking.
  // std::vector<int*> shared_states_;
};
}  // namespace spot
